#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ —Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ HTML (–±–µ–∑ facebook-scraper)
"""
import asyncio
import sys
import os
import re
from datetime import datetime
from typing import Dict, Any, Optional, List

try:
    from bs4 import BeautifulSoup
except ImportError:
    print("‚ùå BeautifulSoup –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install beautifulsoup4")
    sys.exit(1)

try:
    import httpx
except ImportError:
    print("‚ùå httpx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install httpx")
    sys.exit(1)


def load_cookies_dict(cookies_file: str) -> Dict[str, str]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å cookies –∏–∑ —Ñ–∞–π–ª–∞"""
    cookies_dict = {}
    if not os.path.exists(cookies_file):
        return cookies_dict
    
    try:
        with open(cookies_file, 'r') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                parts = line.split('\t')
                if len(parts) >= 7:
                    cookie_name = parts[5]
                    cookie_value = parts[6] if len(parts) > 6 else ''
                    cookies_dict[cookie_name] = cookie_value
    except Exception as e:
        print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å cookies: {e}")
    
    return cookies_dict


def parse_comments_from_html(html_content: str, limit: int = 100) -> Dict[str, Any]:
    """–ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏–∑ HTML"""
    if not html_content or not html_content.strip():
        return {"comments": [], "total_count": 0}
    
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        comments = []
        
        # –†–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
        comment_elements = []
        
        # –í–∞—Ä–∏–∞–Ω—Ç 1: –ü–æ–∏—Å–∫ –ø–æ data-ft
        comment_elements = soup.find_all(attrs={"data-ft": re.compile(r".*top_level_post_id.*")})
        
        # –í–∞—Ä–∏–∞–Ω—Ç 2: –ü–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º
        if not comment_elements:
            comment_elements = soup.find_all('div', class_=re.compile(r'.*comment.*', re.I))
        
        # –í–∞—Ä–∏–∞–Ω—Ç 3: –ü–æ–∏—Å–∫ –ø–æ data-testid
        if not comment_elements:
            comment_elements = soup.find_all('div', attrs={"data-testid": re.compile(r".*comment.*", re.I)})
        
        # –í–∞—Ä–∏–∞–Ω—Ç 4: –ü–æ–∏—Å–∫ –ø–æ role="article"
        if not comment_elements:
            comment_elements = soup.find_all('div', role="article")
        
        # –í–∞—Ä–∏–∞–Ω—Ç 5: –ü–æ–∏—Å–∫ –ø–æ data-sigil
        if not comment_elements:
            comment_elements = soup.find_all(attrs={"data-sigil": re.compile(r".*comment.*", re.I)})
        
        print(f"   –ù–∞–π–¥–µ–Ω–æ {len(comment_elements)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤")
        
        for idx, element in enumerate(comment_elements[:limit]):
            try:
                comment_data = extract_comment_data(element)
                if comment_data and comment_data.get("text"):
                    comments.append(comment_data)
            except Exception as e:
                continue
        
        return {
            "comments": comments,
            "total_count": len(comments)
        }
        
    except Exception as e:
        return {
            "comments": [],
            "total_count": 0,
            "error": str(e)
        }


def extract_comment_data(element) -> Optional[Dict[str, Any]]:
    """–ò–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –æ–¥–Ω–æ–≥–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è"""
    try:
        comment_data = {}
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        text_selectors = [
            'div[data-testid="comment"]',
            '.userContent',
            '[data-sigil="comment-body"]',
            '.comment-body',
            'span[dir="auto"]',
        ]
        
        text = None
        for selector in text_selectors:
            text_elem = element.select_one(selector)
            if text_elem:
                text = text_elem.get_text(strip=True)
                if text:
                    break
        
        if not text:
            text_parts = []
            for text_node in element.find_all(string=True):
                parent = text_node.parent
                if parent and parent.name not in ['a', 'button', 'script', 'style']:
                    text_part = text_node.strip()
                    if text_part:
                        text_parts.append(text_part)
            text = ' '.join(text_parts).strip()
        
        comment_data["text"] = text or ""
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–∞
        author_selectors = [
            'a[role="link"]',
            'strong a',
            'h3 a',
            '[data-hovercard-prefer-more-content-show="1"]',
            'a[href*="/user/"]',
            'a[href*="/profile.php"]',
        ]
        
        author = None
        for selector in author_selectors:
            author_elem = element.select_one(selector)
            if author_elem:
                author = author_elem.get_text(strip=True)
                if author:
                    break
        
        comment_data["author"] = author or ""
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ª–∞–π–∫–æ–≤
        likes = 0
        likes_selectors = [
            '[aria-label*="Like"]',
            '[data-sigil="reactions-count"]',
            '.like-count',
        ]
        
        for selector in likes_selectors:
            likes_elem = element.select_one(selector)
            if likes_elem:
                likes_text = likes_elem.get_text(strip=True)
                likes_match = re.search(r'(\d+)', likes_text.replace(',', '').replace('.', ''))
                if likes_match:
                    try:
                        likes = int(likes_match.group(1))
                        break
                    except ValueError:
                        pass
        
        comment_data["likes"] = likes
        
        return comment_data if comment_data.get("text") else None
        
    except Exception:
        return None


async def fetch_and_parse(url: str, limit: int = 100):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å HTML –∏ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏"""
    print(f"\nüì• –ó–∞–≥—Ä—É–∂–∞–µ–º HTML —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
    
    cookies_file = "cookies.txt"
    cookies_dict = load_cookies_dict(cookies_file)
    
    # –ü—Ä–æ–±—É–µ–º –º–æ–±–∏–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é Facebook (—á–∞—Å—Ç–æ –±–æ–ª–µ–µ –¥–æ—Å—Ç—É–ø–Ω–∞)
    mobile_url = url.replace("www.facebook.com", "m.facebook.com")
    print(f"   –ü—Ä–æ–±—É–µ–º –º–æ–±–∏–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é: {mobile_url}")
    
    headers = {
        "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Cache-Control": "max-age=0",
    }
    
    try:
        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            # –ü—Ä–æ–±—É–µ–º —Å–Ω–∞—á–∞–ª–∞ –º–æ–±–∏–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é
            try:
                response = await client.get(mobile_url, headers=headers, cookies=cookies_dict)
                if response.status_code == 200:
                    print(f"   ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–±–∏–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é")
                else:
                    print(f"   ‚ö†Ô∏è  –ú–æ–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –≤–µ—Ä–Ω—É–ª–∞ {response.status_code}, –ø—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω—É—é")
                    response = await client.get(url, headers=headers, cookies=cookies_dict)
            except:
                # –ï—Å–ª–∏ –º–æ–±–∏–ª—å–Ω–∞—è –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω—É—é
                print(f"   ‚ö†Ô∏è  –ú–æ–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞, –ø—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω—É—é")
                response = await client.get(url, headers=headers, cookies=cookies_dict)
            
            response.raise_for_status()
            
            html_content = response.text
            print(f"   ‚úÖ HTML –∑–∞–≥—Ä—É–∂–µ–Ω: {len(html_content):,} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ü–∞—Ä—Å–∏–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
            print(f"\nüîç –ü–∞—Ä—Å–∏–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏...")
            result = parse_comments_from_html(html_content, limit=limit)
            
            result["url"] = url
            result["html_size"] = len(html_content)
            result["fetched_at"] = datetime.now().isoformat()
            
            return result
            
    except httpx.HTTPStatusError as e:
        print(f"   ‚ùå HTTP –æ—à–∏–±–∫–∞: {e.response.status_code}")
        raise
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
        raise


async def main():
    url = "https://www.facebook.com/premierbankso/posts/pfbid08QZAvQEGniaWzLPvfMGhtebL8ANKEW43weHKW3o9si8Jr9ZGEXSkfPxiHFk5oAR1l"
    
    print("=" * 80)
    print("üß™ –¢–µ—Å—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ —Å Facebook")
    print("=" * 80)
    print(f"URL: {url}")
    print("-" * 80)
    
    if os.path.exists("cookies.txt"):
        print("‚úÖ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª cookies.txt")
    else:
        print("‚ö†Ô∏è  –§–∞–π–ª cookies.txt –Ω–µ –Ω–∞–π–¥–µ–Ω (–º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –¥–ª—è –¥–æ—Å—Ç—É–ø–∞)")
    
    try:
        result = await fetch_and_parse(url, limit=100)
        
        comments = result.get('comments', [])
        print(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ!")
        print(f"   –ù–∞–π–¥–µ–Ω–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {len(comments)}")
        print(f"   –†–∞–∑–º–µ—Ä HTML: {result.get('html_size', 0):,} —Å–∏–º–≤–æ–ª–æ–≤")
        
        if comments:
            print(f"\nüí¨ –ü–µ—Ä–≤—ã–µ {min(10, len(comments))} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤:")
            print("-" * 80)
            for i, comment in enumerate(comments[:10], 1):
                author = comment.get('author', '–ê–Ω–æ–Ω–∏–º')
                text = comment.get('text', '')
                likes = comment.get('likes', 0)
                
                print(f"\n{i}. üë§ {author}")
                if text:
                    preview = text[:200] + "..." if len(text) > 200 else text
                    print(f"   üí≠ {preview}")
                if likes > 0:
                    print(f"   ‚ù§Ô∏è  {likes} –ª–∞–π–∫–æ–≤")
        else:
            print("\n‚ö†Ô∏è –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            if result.get('error'):
                print(f"   –û—à–∏–±–∫–∞: {result.get('error')}")
            print("\nüí° –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
            print("   - Facebook –±–ª–æ–∫–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –±–µ–∑ cookies")
            print("   - –°—Ç—Ä—É–∫—Ç—É—Ä–∞ HTML –∏–∑–º–µ–Ω–∏–ª–∞—Å—å")
            print("   - –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ —Ç—Ä–µ–±—É—é—Ç –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
            print("   - –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ JavaScript")
        
        print("\n" + "=" * 80)
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())

